{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers, callbacks, optimizers, models\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 46\n",
    "classifier = Sequential()\n",
    "classifier.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,1)))\n",
    "classifier.add(layers.MaxPooling2D((2,2)))\n",
    "classifier.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "classifier.add(layers.MaxPooling2D((2,2)))\n",
    "classifier.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
    "classifier.add(layers.Flatten())\n",
    "classifier.add(layers.Dense(num_of_classes, activation='relu'))\n",
    "classifier.add(layers.Dense(num_of_classes, activation='softmax'))\n",
    "classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# opt = optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.99, decay=0.0, amsgrad=False)\n",
    "# classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 78200 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('Train',\n",
    "                                                   target_size=(32, 32),\n",
    "                                                    batch_size=32,\n",
    "                                                    color_mode = 'grayscale',\n",
    "                                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13800 images belonging to 46 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('Test',\n",
    "                                                target_size=(32, 32),\n",
    "                                                \n",
    "                                                color_mode = 'grayscale',\n",
    "                                                batch_size=1,\n",
    "                                                shuffle = False,\n",
    "                                                seed = 2076,\n",
    "                                                class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "2443/2443 [==============================] - 104s 43ms/step - loss: 1.0891 - accuracy: 0.6874 - val_loss: 1.6366e-04 - val_accuracy: 0.8563\n"
     ]
    }
   ],
   "source": [
    "sample_size = training_set.n\n",
    "batch_size = 32\n",
    "history =classifier.fit_generator(training_set,\n",
    "                            steps_per_epoch=sample_size // batch_size,\n",
    "                            epochs=1,\n",
    "                            validation_data=test_set,\n",
    "                            validation_steps=13800,\n",
    "                            \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('anvil.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras_preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# load model\n",
    "model_path = 'new.h5'\n",
    "convnet = load_model(model_path)\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "nepali=['ka','khaa','ga','4','5','6','7','8','9','10','क','12','13','14','15','16','17','18','19','20','21','kha','23','24','25','26','27','28','29','30','31','32','33','34','35','36','o', '१' ,'२' , '३', '४', '५', '६', '७', '८', '९']\n",
    "\n",
    "def predict_character(image_file):\n",
    "    global graph\n",
    "    with graph.as_default():\n",
    "        image_loaded = load_img(image_file,target_size=(32,32),color_mode='grayscale')\n",
    "        img_arr = (img_to_array(image_loaded)/255.0).reshape(1,32,32,1)\n",
    "        probabilities = convnet.predict(img_arr)\n",
    "        pred = np.argmax(probabilities)\n",
    "        return nepali[pred], np.amax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anvil-uplink\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/46/8962b00130fba310654cb674462e3301f347132e8fbb49e01fe7db2e3c2c/anvil_uplink-0.3.26-py2.py3-none-any.whl (49kB)\n",
      "Collecting argparse\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
      "Collecting ws4py==0.3.4\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/60/5d135c8161a2a67d7c227d57bb599fad967d818dbcdca08daa2d60eb87b9/ws4py-0.3.4.tar.gz\n",
      "Requirement already satisfied: future in c:\\users\\krishna\\anaconda3\\lib\\site-packages (from anvil-uplink) (0.17.1)\n",
      "Requirement already satisfied: six in c:\\users\\krishna\\appdata\\roaming\\python\\python37\\site-packages (from anvil-uplink) (1.13.0)\n",
      "Building wheels for collected packages: ws4py\n",
      "  Building wheel for ws4py (setup.py): started\n",
      "  Building wheel for ws4py (setup.py): finished with status 'done'\n",
      "  Created wheel for ws4py: filename=ws4py-0.3.4-cp37-none-any.whl size=41816 sha256=7c1766dec8f8d1f4a78af7d8cd578d3c22f528794abef98480c9656097606c2d\n",
      "  Stored in directory: C:\\Users\\Krishna\\AppData\\Local\\pip\\Cache\\wheels\\19\\1f\\0d\\beff5822af761b66067b5e0b251a9c66af3ae15828ee9a8f15\n",
      "Successfully built ws4py\n",
      "Installing collected packages: argparse, ws4py, anvil-uplink\n",
      "Successfully installed anvil-uplink-0.3.26 argparse-1.4.0 ws4py-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install anvil-uplink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Authenticated OK\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"SRBPI7YIPNBWMTEJTEQT7SYQ-VC4C4VHCHESN7SXI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import anvil.media\n",
    "@anvil.server.callable\n",
    "\n",
    "def classify_image(file):\n",
    "    nepali=['ka','khaa','ga','4','5','6','7','8','9','10','क','12','13','14','15','16','17','18','19','20','21','kha','23','24','25','26','27','28','29','30','31','32','33','34','35','36','o', '१' ,'२' , '३', '४', '५', '६', '७', '८', '९']\n",
    "    with anvil.media.TempFile(file) as filename:\n",
    "        img = load_img(filename)\n",
    "    img = img.resize((1,32,32,1), resample = PIL.Image.BICUBIC)\n",
    "    img_arr = img_to_array(img)\n",
    "    probabilities = convnet.predict(img_arr)\n",
    "    pred = np.argmax(probabilities)\n",
    "    return nepali[pred], np.amax(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
